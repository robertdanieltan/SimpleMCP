# =============================================================================
# AI Agent MCP Service Learning Project - Environment Configuration
# =============================================================================
# Copy this file to .env and update with your actual values
# IMPORTANT: Never commit your .env file to version control!

# =============================================================================
# LLM Provider Configuration (Unified Approach - RECOMMENDED)
# =============================================================================
# This is the new, simplified way to configure LLM providers.
# Choose one provider and configure it with generic variables.

# Primary LLM Provider Selection
# Supported values: gemini, openai, anthropic, ollama
LLM_PROVIDER=gemini

# Generic LLM Configuration (applies to the selected provider)
# Get API keys from:
# - Gemini: https://makersuite.google.com/app/apikey
# - OpenAI: https://platform.openai.com/api-keys  
# - Anthropic: https://console.anthropic.com/
LLM_API_KEY=your_api_key_here

# Model selection (provider-specific defaults will be used if not specified)
# - Gemini: gemini-pro, gemini-pro-vision
# - OpenAI: gpt-3.5-turbo, gpt-4, gpt-4-turbo
# - Anthropic: claude-3-sonnet-20240229, claude-3-opus-20240229
# - Ollama: llama2, codellama, mistral (requires Ollama running locally)
LLM_MODEL=gemini-pro

# Common LLM Parameters
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=1000
LLM_TIMEOUT=30

# Optional: Provider-specific settings
# For OpenAI custom endpoints, Ollama base URL, etc.
# LLM_BASE_URL=http://localhost:11434

# For OpenAI organization (optional)
# LLM_ORGANIZATION=your_org_id_here

# For Gemini safety settings (optional)
# LLM_SAFETY_SETTINGS=default

# For Ollama keep-alive setting (optional)
# LLM_KEEP_ALIVE=5m

# For retry configuration (optional)
# LLM_MAX_RETRIES=3

# For Ollama num_predict setting (optional, -1 means no limit)
# LLM_NUM_PREDICT=-1

# For mock provider testing (optional)
# LLM_SIMULATE_DELAY=0.1
# LLM_FAILURE_RATE=0.0

# =============================================================================
# Legacy Provider Configuration (DEPRECATED - for backward compatibility)
# =============================================================================
# The old provider-specific approach is still supported but deprecated.
# Use the unified approach above for new deployments.

# Gemini API Configuration (LEGACY)
# GEMINI_API_KEY=your_gemini_api_key_here
# GEMINI_MODEL=gemini-pro
# GEMINI_TEMPERATURE=0.7
# GEMINI_MAX_TOKENS=1000
# GEMINI_TIMEOUT=30

# OpenAI API Configuration (LEGACY)
# OPENAI_API_KEY=your_openai_api_key_here
# OPENAI_MODEL=gpt-3.5-turbo
# OPENAI_TEMPERATURE=0.7
# OPENAI_MAX_TOKENS=1000
# OPENAI_TIMEOUT=30
# OPENAI_ORGANIZATION=your_org_id_here

# Anthropic API Configuration (LEGACY)
# ANTHROPIC_API_KEY=your_anthropic_api_key_here
# ANTHROPIC_MODEL=claude-3-sonnet-20240229
# ANTHROPIC_TEMPERATURE=0.7
# ANTHROPIC_MAX_TOKENS=1000
# ANTHROPIC_TIMEOUT=30

# Ollama Local LLM Configuration (LEGACY)
# OLLAMA_ENABLED=true
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama2
# OLLAMA_TEMPERATURE=0.7
# OLLAMA_MAX_TOKENS=1000
# OLLAMA_TIMEOUT=60
# OLLAMA_KEEP_ALIVE=5m

# LLM Session Configuration
LLM_SESSION_TIMEOUT_HOURS=24
LLM_MAX_CONCURRENT_SESSIONS=100

# =============================================================================
# Database Configuration
# =============================================================================
# PostgreSQL database settings
POSTGRES_USER=postgres
POSTGRES_PASSWORD=secure_password_here
POSTGRES_DB=aiagent_mcp

# Database URL for application connections (auto-constructed)
# Format: postgresql://username:password@host:port/database
DATABASE_URL=postgresql://postgres:secure_password_here@postgres:5432/aiagent_mcp

# =============================================================================
# pgAdmin Configuration
# =============================================================================
# Web interface credentials for database administration
# Access pgAdmin at: http://localhost:8080
PGADMIN_DEFAULT_EMAIL=admin@example.com
PGADMIN_DEFAULT_PASSWORD=admin_password_here

# =============================================================================
# Service Configuration
# =============================================================================
# Internal service URLs (used by containers for communication)
MCP_SERVICE_URL=http://mcp-service:8001
AI_AGENT_SERVICE_URL=http://ai-agent:8000

# Service ports (change if you have conflicts)
AI_AGENT_PORT=8000
MCP_SERVICE_PORT=8001
POSTGRES_PORT=5432
PGADMIN_PORT=8080

# =============================================================================
# Development Settings
# =============================================================================
# Environment mode: 'development' or 'production'
ENVIRONMENT=development

# Logging configuration
LOG_LEVEL=INFO
DEBUG=true

# Database connection settings
DB_POOL_SIZE=5
DB_MAX_OVERFLOW=10
DB_POOL_TIMEOUT=30

# =============================================================================
# Optional: Advanced Configuration
# =============================================================================
# Uncomment and modify these settings if needed

# Custom database host (for external PostgreSQL)
# POSTGRES_HOST=localhost

# Custom Gemini API settings (now configured in LLM Provider section above)

# Health check intervals (in seconds)
# HEALTH_CHECK_INTERVAL=30
# HEALTH_CHECK_TIMEOUT=10
# HEALTH_CHECK_RETRIES=3

# =============================================================================
# Security Notes
# =============================================================================
# 1. Change all default passwords before production use
# 2. Use strong, unique passwords for all services
# 3. Keep your Gemini API key secure and never share it
# 4. Consider using Docker secrets for production deployments
# 5. Regularly rotate API keys and passwords